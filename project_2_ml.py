# -*- coding: utf-8 -*-
"""Project_2_ML

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGklvEnCrh2e70pEPqweePL1tfYXUAuu
"""

# Gradient Boosting from Scratch - Google Colab Ready

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons

# ----------- Decision Tree (Regression Tree for Residuals) -----------

class DecisionTree:
    def __init__(self, max_depth=1, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def fit(self, X, y, depth=0):
        n_samples, n_features = X.shape
        if depth >= self.max_depth or n_samples < self.min_samples_split:
            return np.mean(y)

        best_split = self._find_best_split(X, y, n_features)
        if not best_split:
            return np.mean(y)

        feature, threshold = best_split
        left_idx = X[:, feature] <= threshold
        right_idx = ~left_idx

        return {
            'feature': feature,
            'threshold': threshold,
            'left': self.fit(X[left_idx], y[left_idx], depth + 1),
            'right': self.fit(X[right_idx], y[right_idx], depth + 1)
        }

    def _find_best_split(self, X, y, n_features):
        best_var = float('inf')
        best_split = None
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                left = y[X[:, feature] <= threshold]
                right = y[X[:, feature] > threshold]
                if len(left) < self.min_samples_split or len(right) < self.min_samples_split:
                    continue
                var = np.var(left) * len(left) + np.var(right) * len(right)
                if var < best_var:
                    best_var = var
                    best_split = (feature, threshold)
        return best_split

    def predict_sample(self, node, x):
        if not isinstance(node, dict):
            return node
        if x[node['feature']] <= node['threshold']:
            return self.predict_sample(node['left'], x)
        else:
            return self.predict_sample(node['right'], x)

    def predict(self, X):
        return np.array([self.predict_sample(self.tree, x) for x in X])

# ----------- Logistic Loss & Gradient Function -----------

class LogisticLoss:
    def gradient(self, y_true, y_pred):
        return -y_true / (1 + np.exp(y_true * y_pred))

    def loss(self, y_true, y_pred):
        return np.log(1 + np.exp(-y_true * y_pred)).mean()

# ----------- Gradient Boosting Classifier Implementation -----------

class GradientBoostingClassifier:
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1, min_samples_split=2):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.trees = []
        self.loss = LogisticLoss()

    def fit(self, X, y):
        y = np.where(y == 0, -1, 1)  # Convert to {-1, 1}
        self.F0 = 0.0
        self.pred = np.full(y.shape, self.F0)

        for i in range(self.n_estimators):
            grad = self.loss.gradient(y, self.pred)
            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)
            tree.tree = tree.fit(X, -grad)
            update = tree.predict(X)
            self.pred += self.learning_rate * update
            self.trees.append(tree)

    def predict_proba(self, X):
        pred = np.full(X.shape[0], self.F0)
        for tree in self.trees:
            pred += self.learning_rate * tree.predict(X)
        proba = 1 / (1 + np.exp(-pred))
        return np.vstack([1 - proba, proba]).T

    def predict(self, X):
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)

# ----------- Synthetic Data + Visualization -----------

# Generate synthetic classification data
X, y = make_moons(n_samples=300, noise=0.25, random_state=42)

# Train the model
model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=2)
model.fit(X, y)
y_pred = model.predict(X)

# Accuracy
accuracy = np.mean(y_pred == y)
print(f"Training Accuracy: {accuracy:.4f}")

# Plot decision boundary
def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                         np.linspace(y_min, y_max, 200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.RdBu)
    plt.title("Gradient Boosting Decision Boundary")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.show()

plot_decision_boundary(X, y, model)